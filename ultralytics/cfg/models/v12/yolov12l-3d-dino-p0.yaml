# YOLOv12m-3D with DINOv3 Single-Scale Integration (P0 only)
# Integration of DINOv3 (Self-supervised Vision Transformer) at P0 (early features) only
# Model size: Large (l)
# DINOv3 levels: P0 only

nc: 8 # number of classes (KITTI: Car, Van, Truck, Pedestrian, Person_sitting, Cyclist, Tram, Misc)
scales: # model compound scaling constants
  # [depth, width, max_channels]
  l: [1.00, 1.00, 512]  # YOLOv12l-3D-Large

# YOLOv12-3D-DINOv3 backbone with single-scale DINOv3 integration at P0
backbone:
  # [from, repeats, module, args]
  # Early stage (P0/2)
  - [-1, 1, Conv,  [64, 3, 2]] # 0-P1/2

  # DINOv3 integration at P0 level (early features only)
  # DINOv3 provides self-supervised feature enhancement for better semantic understanding
  - [-1, 1, DINO3Backbone, ['dinov3_vitb16', True, 64]]  # 1: DINOv3-enhanced P0 features

  # Continue with standard backbone after DINO P0
  - [-1, 1, Conv,  [128, 3, 2, 1, 2]] # 2-P2/4
  - [-1, 2, C3k2,  [256, False, 0.25]]
  - [-1, 1, Conv,  [256, 3, 2, 1, 4]] # 4-P3/8
  - [-1, 2, C3k2,  [512, False, 0.25]]
  - [-1, 1, Conv,  [512, 3, 2]] # 7-P4/16
  - [-1, 4, A2C2f, [512, True, 4]]
  - [-1, 1, Conv,  [1024, 3, 2]] # 9-P5/32
  - [-1, 4, A2C2f, [1024, True, 1]] # 10

# YOLOv12-3D head (with Detect3D for 3D predictions)
head:
  - [-1, 1, nn.Upsample, [None, 2, "nearest"]]  # 10
  - [[-1, 7], 1, Concat, [1]] # 11: cat backbone P4 (layer 7)
  - [-1, 2, A2C2f, [512, False, -1]] # 12

  - [-1, 1, nn.Upsample, [None, 2, "nearest"]]  # 13
  - [[-1, 5], 1, Concat, [1]] # 14: cat backbone P3 (layer 5)
  - [-1, 2, A2C2f, [256, False, -1]] # 15 (P3)

  - [-1, 1, Conv, [256, 3, 2]]  # 16
  - [[-1, 12], 1, Concat, [1]] # 17: cat head P4
  - [-1, 2, A2C2f, [512, False, -1]] # 18 (P4)

  - [-1, 1, Conv, [512, 3, 2]]  # 19
  - [[-1, 9], 1, Concat, [1]] # 20: cat head P5 (layer 9)
  - [-1, 2, C3k2, [1024, True]] # 21 (P5/32-large)

  - [[15, 18, 21], 1, Detect3D, [nc]] # 22: Detect3D(P3, P4, P5) - 3D detection head
